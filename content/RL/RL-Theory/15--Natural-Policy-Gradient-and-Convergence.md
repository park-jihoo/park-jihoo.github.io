---
id: a6b41203-70cc-4fb0-8164-a7cbc17ca864
title: 15. Natural Policy Gradient and Convergence
created_time: 2023-07-27T04:30:00.000Z
last_edited_time: 2023-08-17T06:46:00.000Z
cover_image: https://www.notion.so/images/page-cover/gradients_11.jpg
하위 항목: []
subclass: RL Theory
class: RL
작성일시: 2023-07-27T04:30:00.000Z
pdf:
  - https://wensun.github.io/CS6789_data_fall_2021/PG_OPT2_annotated.pdf
  - https://wensun.github.io/CS6789_data_fall_2021/PG_OPT3_annotated.pdf
상위 항목: []
_thumbnail: https://www.notion.so/images/page-cover/gradients_11.jpg

---

> 💡 Do the PG methods globally converge to optimal policy?

# The Natural Policy Gradient

*   Recall that the Fisher information matrix of a parametrized density p\_\theta(x) is defined as \mathbb E\_{x\sim p\_\theta}\[\nabla\log p\_\theta(x)\nabla\log p\_\theta(x)^\top]

*   Define \mathscr{F}*\rho^\thetaas the Fisher matrix on the family of distributions {\pi*\theta(\cdot|s)|s\in S} as

    ```undefined
    \mathscr{F}_\rho^\theta=\mathbb{E}_{s\sim d_\rho^{\pi_\theta}}\left[\mathbb{E}_{a\sim\pi_\theta(\cdot|s)}\left[(\nabla\log\pi_\theta(a|s))\nabla\log\pi_\theta(a|s)^\top\right]\right]
    ```

*   The NPG algorithm performs a gradient updates in this induced geometry

    ```undefined
    \theta^{(t+1)}=\theta^{(t)}+\eta F_\rho(\theta^{(t)})^\dag\nabla_\theta V^{(t)}(\rho)
    ```

    where M^\dag denotes the [Moore-Pensore psuedoinverse](https://en.wikipedia.org/wiki/Moore–Penrose_inverse) of M

# Compatible Function Approximation

*   Let w^\* denote the following minimizer of the **compatible function approximation** error

    ```undefined
    w^*\in\argmin_w\mathbb{E}_{s\sim d_\mu^{\pi_\theta}}\left[\mathbb{E}_{a\sim\pi_\theta(\cdot|s)}\left[\left(A^{\pi_\theta}(s,a)-w\cdot\nabla_\theta\log\pi_\theta(a|s)\right)^2\right]\right]
    ```

*   Lemma: Let \hat A^{\pi\_\theta}(s,a) be the best linear predictor of A^{\pi\_\theta}(s,a) using \nabla\_\theta\log\pi\_\theta(a|s), i.e \hat A^{\pi\_\theta}(s,a):=w^\*\cdot\nabla\_\theta\log\pi\_\theta(a|s). We have

    ```undefined
    \nabla_\theta V^{\pi_\theta}(\mu)=\frac1{1-\gamma}\mathbb{E}_{s\sim d_\mu^{\pi_\theta}}\left[\mathbb{E}_{a\sim\pi_\theta(\cdot|s)}\left[\nabla_\theta\log\pi_\theta(a|s)\hat A^{\pi_\theta}(s,a)\right]\right]
    ```

    we can use \hat A^{\pi\_\theta}(s,a) instead of A^{\pi\_\theta}(s,a)

## NPG + Compatible Function Approximation

*   Lemma: We have that F\_\mu(\theta)^\dag\nabla\_\theta V^\theta(\mu)=\frac1{1-\gamma}w^*, the NPG direction is the weights w^*

*   Proof: rearrange first order optimality conditions for w^\* and then rearrange.

# NPG Softmax Case

*   Lemma(Softmax NPG as soft policy iteration) The NPG update is

    ```undefined
    \theta^{(t+1)}=\theta^{(t)}+\frac\eta{1-\gamma}A^{(t)}
    ```

    and this leads to the update

    ```undefined
    \pi^{(t+1)}(a|s)=\pi^{(t)}(a|s)\frac{\exp(\eta A^{(t)}(s,a)/(1-\gamma))}{Z_t(s)}
    ```

    where

    ```undefined
    Z_t(s)=\sum_a\pi^{(t)}(a|s)\exp(\eta A^{(t)}(s,a)/(1-\gamma))
    ```

# Global convergence for NPG

*   Params \theta^{(0)}=0 and \eta>0, For all \rho and T>0, we have

    ```undefined
    V^{(T)}(\rho)\ge V^*(\rho)-\frac{\log A}{\eta T}-\frac1{(1-\gamma)^2T}
    ```

*   Setting \eta\ge(1-\gamma)^2\log A, NPG finds an \epsilon-optimal policy when T\ge\frac2{(1-\gamma)^2\epsilon}

*   Iteration complexity has

    *   No dimension dependence

    *   No dependence on start state measure \rho

    *   No **flat gradient** problem

## Improvement Lower Bound

*   For the iterates \pi^{(t)} generated by the NPG, we have for all distributions \mu

    ```undefined
    V^{(t+1)}(\mu)-V^{(t)}(\mu)\ge\frac{(1-\gamma)}\eta\mathbb E_{s\sim\mu}\log Z_t(s)>0
    ```

# NPG & Log Linear Policy Classes

*   Feature vector \phi(s,a)\in\R^d, \pi\_\theta(a|s)=\frac{\exp(\theta^\top\phi(s,a))}{\sum\_{a'}\exp(\theta^\top\phi(s,a'))}

*   We have \nabla\_\theta\log\pi\_\theta(a|s)=\bar\phi\_{s,a}^\theta, where \bar\phi\_{s,a}^\theta = \phi\_{s,a}-\mathbb E\_{a'\sim\pi\_\theta(\cdot|s)}\[\phi\_{s,a}]

*   Apply NPG update: \theta\leftarrow\theta+\eta w\_*, w\_*\in\argmin\_w \mathbb E\_{s\sim d\_\rho^{\pi\_\theta},a\sim\pi\_\theta(\cdot|s)}\left\[\left(A^{\pi\_\theta}(s,a)-w\cdot\bar\phi\_{s,a}^\theta\right)^2\right]

*   Equivalently, for same w\_\*,

    ```undefined
    \pi(a|s)\leftarrow\frac{\pi(a|s)\exp(w_*\cdot\phi_{s,a})}{Z_s}
    ```

# Q-NPG: use Q rather A

*   Apply NPG update: \theta\leftarrow\theta+\eta w\_*, w\_*\in\argmin\_w \mathbb E\_{s\sim d\_\rho^{\pi\_\theta},a\sim\pi\_\theta(\cdot|s)}\left\[\left(Q^{\pi\_\theta}(s,a)-w\cdot\bar\phi\_{s,a}^\theta\right)^2\right]

*   Equivalently, for same w\_\*,

    ```undefined
    \pi(a|s)\leftarrow\frac{\pi(a|s)\exp(w_*\cdot\phi_{s,a})}{Z_s}
    ```

## Q-NPG with Starting Measure

*   For state-action distribution D, define

    ```undefined
    L(w;\theta,D):=\mathbb E_{s,a\sim D}\left[\left(Q^{\pi_\theta}(s,a)-w\cdot\phi_{s,a}\right)^2\right]
    ```

*   The approximate version \theta^{(t+1)} = \theta^{(t)}+\eta w^{(t)}, where w^{(t)} \approx\argmin\_{|w|\_2\le W}L(w;\theta^{(t)},d^{(t)})

*   Equivalently,

    ```undefined
    \pi^{(t+1)}(a|s)\leftarrow\frac{\pi^{(t)}(a|s)\exp(w^{(t)}\cdot\phi_{s,a})}{Z_s}
    ```

# Generic Perturbation Analysis of NPG

## NPG regret lemma

*   Fix any comparison policy \tilde\pi and any state distribution \rho. Assume \log\pi\_\theta(a|s) is \beta-smooth function of \theta. Let

    ```undefined
    \text{err}_t = \mathbb E_{s\sim\tilde d}\left[\mathbb E _{a\sim\tilde\pi(\cdot|s)}\left[A^{(t)}(s,a)-w^{(t)}\cdot\nabla_\theta\log\pi^{(t)}(a|s)\right]\right]
    ```

    then we have

    ```undefined
    \min_{t<T}\left\{V^{\tilde\pi}(\rho)-V^{(t)}(\rho)\right\}\le\frac 1{1-\gamma}\left(W\sqrt{\frac{2\beta\log A}T}+\frac1T\sum_{t=0}^{T-1}\text{err}_t\right)
    ```

    (where we set using \eta=\sqrt{2\log A/(\beta W^2 T)}

## Approximate Q-NPG

*   The approximate version:

    ```undefined
    \theta^{(t+1)}=\theta^{(t)}+\eta w^{(t)},\ \text{ where }w^{(t)}\approx\argmin_{\|w\|_2\le W}L(w;\theta^{(t)},d^{(t)})
    ```

*   Error decomposition

    ```undefined
    L(w^{(t)};\theta^{(t)},d^{(t)})=\underbrace{L(w^{(t)};\theta^{(t)},d^{(t)})-L(w_*^{(t)};\theta^{(t)},d^{(t)})}_{\text{Excess risk}}+\underbrace{L(w_*^{(t)};\theta^{(t)},d^{(t)})}_{\text{Approximation error}}
    ```

## Q-NPG Conv Rate with Estimation error

*   Suppose no Approximation error, and excess risk less then \epsilon\_{\text{stat}}

*   Conditioning: suppose |\phi\_{s,a}|\_2\le 1 and for the initial measure \nu,

    ```undefined
    \sigma_{\min}\left(\mathbb E_{s,a\sim\nu}\left[\phi_{s,a}\phi_{s,a}^\top\right]\right)=\lambda_{\min},\ \kappa=1/\lambda
    ```

*   Fix any state distribution \rho, any comparator policy \pi^\*. With \eta set appropriately and under the above assumptions, we have that

    ```undefined
    \mathbb E\left[\min_{t<T}\left\{V^{\tilde\pi}(\rho)-V^{(t)}(\rho)\right\}\right]\le\frac W{1-\gamma}\sqrt{\frac{2\log A}T}+\sqrt{\frac{4A}{(1-\gamma)^3}\left(\kappa\cdot\epsilon_{\text{stat}}\right)}
    ```

## Q-NPG Conv Rate with Approximation error and estimation error

*   Fix any state distribution \rho, any comparator policy \pi^\*. With \eta set appropriately and under the above assumptions, we have that

    ```undefined
    \mathbb E\left[\min_{t<T}\left\{V^{\tilde\pi}(\rho)-V^{(t)}(\rho)\right\}\right]\le\frac {BW}{1-\gamma}\sqrt{\frac{2\log A}T}+\sqrt{\frac{4A}{(1-\gamma)^3}\left(\kappa\cdot\epsilon_{\text{stat}}+\left\|\frac{d^*}{\nu}\right\|_\infty\cdot\epsilon_{\text{approx}}\right)}
    ```

# NPG & Neural Policy Class

*   Neural net f\_\theta:S\times A\rightarrow\R, Policy \pi\_\theta(a|s)=\frac{\exp(f\_\theta(s,a))}{\sum\_{a'}\exp{f\_\theta(s,a')}}

*   We have \nabla\_\theta\log\pi\_\theta(a|s)=g\_\theta(s,a), where g\_\theta(s,a) = \nabla\_\theta f\_\theta(s,a)-\mathbb E\_{a'\sim\pi\_\theta(\cdot|s)}\[\nabla\_\theta f\_\theta(s,a')]

*   The NPG update rule is  \theta\leftarrow\theta+\eta w\_\* with

    ```undefined
    w_*\in\argmin_w\mathbb E_{s\sim d_\rho^{\pi_\theta},a\sim\pi_\theta(\cdot|s)}\left[\left(A^{\pi_\theta}(s,a)-w\cdot g_\theta(s,a)\right)^2\right]
    ```
