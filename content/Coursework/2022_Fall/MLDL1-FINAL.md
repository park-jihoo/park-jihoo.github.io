---
id: c4fe8e99-0760-4c24-b152-66e376d28bc5
title: MLDL1 FINAL
created_time: 2022-12-07T09:47:00.000Z
last_edited_time: 2023-10-07T16:40:00.000Z
cover_image: https://www.notion.so/images/page-cover/gradients_11.jpg
icon_emoji: ğŸ¾
í•˜ìœ„ í•­ëª©: []
subclass: 2022_Fall
class: Coursework
ì‘ì„±ì¼ì‹œ: 2022-12-07T09:47:00.000Z
ìƒìœ„ í•­ëª©: []
_thumbnail: https://www.notion.so/images/page-cover/gradients_11.jpg

---

# Nearest Neighbor Classifiers

    - Nearest Neighborìœ¼ë¡œ classificationí•  ì‹œì˜ ë‹¨ì : O(1) for training but O(N) for testing

    - L_1 (A,B) = \sum_{\substack {i,j}}|A_{i,j}-B_{i,j}|, L_2(A,B) = {\sqrt{(A_{i,j}-B_{i,j})^2}}

    - However, nearest neighbors with pixel distance are never used

    - Curse of dimensionality : need exponentially increasing number of examples for higher dimensional data

***

# Linear Softmax Classifier

    - Linear Classifier: weighted sum of input pixels

    - f({\textsf x}, W) = {\textsf y} = Wx+b = \begin{bmatrix} W & b \end{bmatrix} \begin{bmatrix}x \\ 1\end{bmatrix} = W^{'}x^{'}

    - bë¼ëŠ” variableì´ í•„ìš”í•œ ì´ìœ : affecting the output without interacting with data {\textsf x}

    - Limations of linear classifier: unbounded and itâ€™s hard to interpret â†’ probabilityë¡œ ë§Œë“¤ê¸°

    - Sigmoid function \sigma(s) = \frac 1 {1+e^{-s}}

    - Sigmoidë¥¼ nìœ¼ë¡œ í™•ì¥í•˜ë©´ softmaxì´ê³ , p(y=c_i |x) = \frac {e^{s_i}}{\sum_j {e^{s_j}}}

    - We can update the parameters based on Loss functions

    - Loss functionì˜ ì¢…ë¥˜: Margin-based(0-1, log loss, exponential loss, hinge loss)

    	- hinge loss and log loss are widely used, but we canâ€™t differentiate hinge loss at 1 and log loss is more interpretable

    - Cross Entropy: {\cal L} = - \frac 1 N  \sum_{i=1}^N \sum_{k=1}^K y_{ik} \log(\hat{y}_{ik}) = -\frac 1 N \sum_{i=1}^{N} \log(\hat{y}_i T_i)

    - Kullback-Leibler divergence D_{KL}(P||Q) = \sum_i P(i) \log \frac {P(i)} {Q(i)}

    - How to follow the slope? Calculate gradient descent \theta _i^{new} = \theta _i^{old}-\alpha \frac {\partial}{\partial \theta_i^{old}} J(\Theta)

    	- Problems: Surface may not be convex and works if and only if the cost function is differentiable

    	- Stochastic Gradient Descent: compute gradients for only on a randomly sampled subset

***

# Neural Networks and Backpropagation

    - Issues with linear classifiers: learn one template and only works if inputs are linearly separable

    - Perceptron: Takes input value and each of them is multiplied with a weight y = \operatorname f(w_1x_1+w_2x_2) and multiple perceptronì—ì„œëŠ” ì´ê²Œ ì—¬ëŸ¬ ë²ˆ ì´ë£¨ì–´ì§

    - Multilayer Perceptrion x \in \reals^d   W_1 \in \reals ^{h \times d}   W_2 \in \reals ^{c \times h}      s \in \reals ^c

    - How Can we add nonlinearity? Activation functions! f(x) = a_2 (W_2a_1(W_1x))

    - Activation Functions: sigmoid, tanh(\tanh (x)), ReLU(\max (0,x))

    - For Backpropagation, we compute gradient of loss \frac {\partial \cal {L}}{\partial \hat W_2} = \frac {\partial \cal {L}}{\partial \hat y} \frac {\partial \hat y}{\partial W_2}, \frac {\partial \cal {L}}{\partial W_1} = \frac {\partial \cal {L}}{\partial \hat y}\frac {\partial \hat {y}}{\partial h}\frac {\partial h}{\partial W_1}

    ```python
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim

    class Net(nn.Module):
    		def __init__(self):
            super(Net, self).__init__()
            self.fc1 = nn.Linear(784, 128)
            self.fc2 = nn.Linear(128, 64)
            self.classifier = nn.Linear(64, 10)
        
        def forward(self, x):
            x = x.reshape(-1, 784)
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.classifier(x)
            return x

    network = Net()
    ```

***

# Convolution Neural Networks

    - Fully Connected Layerì´ ê°€ì§€ëŠ” ë¬¸ì œ: models relationships from every input value to every output value, so no spatial locality and positional invariance â†’ CNN!

    - Convolutional Layer 

    	> Slide over the image spatially, computing dot products

    - Why Nested Conv Layer? Get Low, Mid, High Level Features

    - Stride: How much pixels to slide, output size \frac {(N-F)}{\mathsf {stride}}+1

    - Padding: zero pad the border, output size \frac {(N-F+2P)}{\mathsf {stride}}+1

    - Given an input volume W \times H\times C, filters K, stride S, zero padding P will produce an output of size W'\times H' \times K where W' = (W-F+2P)/S + 1, H' = (H-F+2P)/S+1, numbers of parameters K(F^2C+1)

    - Pooling: for downsampling and denoising to control overfit. Max poolì—ì„œëŠ” filterì˜ maxê°’ì„ ê³ ë¥´ê³ , average poolingì—ì„œëŠ” filterì˜ averageê°’ì„ êµ¬í•œë‹¤.

***

# Training Neural Networks

    - Activation Functions

    	- Sigmoid Function: Kill the gradients, outputs are not zero-centered, computationally expensive

    	- Not zero-centered outputì´ ë¬¸ì œì¸ ì´ìœ : sigmoid node doesnâ€™t change the sign of the upstream gradient for all w_i

    	- Tanh function: zero-centeredì´ê¸´ í•˜ì§€ë§Œ, killing gradientì— ëŒ€í•œ ë¬¸ì œëŠ” ê°€ì§€ê³  ìˆìŒ

    	- ReLU: Not zero centered, anad not differentiable when x=0, if an initial output is inegative, itâ€™s never updated

    - Why zero-centering? Classification becomes less sensitive and easy to optimize

    - Why normalize? Each axis has the same importance

    ```python
    X -= np.mean(X, axis=0) #zero-centering
    X /= np.std(X, axis=0) #Normalizing
    ```

    - Data augmentation: slightly modifying each datum without affecting its semantics

    	- Horizontal flips, Random crops, Scaling, Color jitter ë“±ì´ ìˆìŒ

    - Weight Initialization

    	- Gaussian Randomì˜ ë¬¸ì œì : gradientê°€ zeroê°€ ë˜ì–´ learningì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ

    	- Xavier Initialization: Var[W_{ij}] = 1/d_{in} ìœ¼ë¡œ ë§ì¶”ì–´ input varianceì™€ output varianceë¥¼ ë™ì¼í•˜ë„ë¡ ë§Œë“œëŠ” ë°©ë²•

    	```python
    	W = np.random.randn(d_in, d_out) / np.sqrt(d_in)
    	```

    - Learning Rate Scheduling: ë„ˆë¬´ ë†’ìœ¼ë©´ explode, ë„ˆë¬´ ë‚®ìœ¼ë©´ ì²œì²œíˆ learningí•¨. ê·¸ë ‡ê¸° ë–„ë¬¸ì— ì²˜ìŒì—ëŠ” ë†’ì˜€ë‹¤ê°€ ê·¸ ë‹¤ìŒì—ëŠ” ë‚®ì¶”ëŠ” decay í˜•ì‹ì„ ì§„í–‰í•˜ë ¤ê³  í•¨

    - Regularization: adding an additional penalty term in the objective function, for example Ridge or Lasso

    	- In a neural network, there is weight decay.\Omega(W) = \sum_i \sum_j W_{i,j}^2 or \Omega(W) = \sum_i \sum_j |W_{i,j}| 

    - Early Stopping: validation lossê°€ ìµœì†Œê°€ ë‚˜ì˜¨ ì§€ ê½¤ ë˜ì—ˆë‹¤ë©´, ë¹ ë¥´ê²Œ stopí•˜ëŠ” ë°©ì‹ì„

    - Dropout: forward ê³¼ì •ì—ì„œ randomly set some neurons to zero, but testì‹œì—ëŠ” ëª¨ë“  neuronì„ í™œì„±í™”í•´ì•¼ í•¨

    - Optimization

    	- Stochastic Gradient Descentì˜ ë¬¸ì œì : progress slow, saddle point, may not be accurate because of mini-batch

    	- SGD+Momentum(ê´€ì„±) v_{t+1} = \rho v_t - \alpha \nabla f(x_t), x_{t+1} = x_t + v_{t+1}

    	- Adagrad, RmsProp: learning rateë¥¼ dimension vectorì— ëŒ€í•´ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ë©°, RmspropëŠ” ì—¬ê¸°ì— decayingì„ ì¶”ê°€í•œ ê²ƒ

    	```python
    	grad_squared = 0.0
    	while True:
    		dx = compute_gradient(x)
    		if adagrad = True:
    			grad_squared += dx*dx
    		elif rmsprop = True:
    			grad_squared = dr * grad_squared + (1-dr)* dx*dx
    		divider = np.sqrt(grad_squared) + 1e-7
    		x -= learning_rate * dx / divider
    	```

    	- ADAM: Rmspropì— SGD+Momentum ê¸°ë²•ì„ ë‘˜ ë‹¤ í™œìš©í•˜ëŠ” ê²ƒ

    	- Second-order Optimization: Use gradient and Hessian to form quadratic approximation, \theta^* = \theta_0 - H^{-1} \nabla_\theta J(\theta_0) where J(\theta) \approx J(\theta_0) + (\theta - \theta_0)^{\intercal}\nabla_\theta J(\theta_0) + \frac 1 2 (\theta - \theta_0)^{\intercal} H(\theta - \theta_0) â†’ takes too long time to calculate

     

    - Batch normalization: In some hidden layers, we can make each dimension have a zero-mean unit variance: \hat x^{(k)} = \frac {x^{(k)} - {\mathbb E}[x^{(k)}]} {\sqrt {Var[x^{(k)}]}}

    - Collate_function: converts PIL images, labels to pytorch tensors

    	```python
    	def collate_fn(data_samples):
    	    batch_x, batch_y = [], []
    	    for x, y in data_samples:
    	      x = transforms.ToTensor()(x)
    	      y = torch.Tensor([y])
    	      batch_x.append(x)
    	      batch_y.append(y)
    	    batch_x = torch.stack(batch_x).float()
    	    batch_y = torch.cat(batch_y).long()
    	    return (batch_x, batch_y)
    	```

***

# Sequential Data & RNN

    - Problem types: one-to-one, many-to-one, many-to-many, one-to-many

    - Word embeddings: change the word as a vector to reflect its semantics. Theyâ€™re fitted to maximize the likelihood L(\theta) = \displaystyle{\prod_{{i=1}}^N \prod_{ -m \le j \le m, j \not = 0 }} p(w_{t+j}|w_t;\theta)

    - CNNì´ sequential dataì—ì„œ í™œìš©í•˜ê¸° ì–´ë ¤ìš´ ì´ìœ : Sentences have variable length ğŸ˜¨

    - Encoder: input as entire sequence, and embeds entire sequence preserving its overall semantics and it will make them arbitrary length

    ```python
    MAX_LEN = 32
    #collate_batchë¥¼ í†µí•´ sentence lengthë¥¼ MAX_LENìœ¼ë¡œ ë§ì¶œ ìˆ˜ ìˆìŒ
    def collate_batch(batch):
        label_list, text_list = [], []
        for (_label, _text) in batch:
          label_list.append(label_pipeline(_label))
          processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)
          if processed_text.size(0) > MAX_LEN:
            processed_text = processed_text[:MAX_LEN]
          else:
            processed_text = torch.cat([processed_text, torch.zeros(MAX_LEN-processed_text.size(0))])
          text_list.append(processed_text)
        label_list = torch.tensor(label_list, dtype=torch.int64)
        text_list= torch.stack(text_list).long()
        return label_list.to(DEVICE), text_list.to(DEVICE)
    ```

    - Recurrent Neural Network: They maintain an internal state, so the new internal state is determined by its old state as well as the input. h_t = f_W(h_{t-1},x_t) = tanh(W_{hh}h_{t-1}+W_{xh}x_t)

    	- Forward pass: all parameters are fixed and they are used to compute \hat y

    	- Backpropagation: from the  loss(diff(\hat y, y)), parameters are updated

    - Language model: probability for a sequence of words, traditionally used Markov assumptions

    - RNNs calculate the likelihood of each word appearing in the next position. ì´ ë•Œ cross entropy loss(J^{(t)}(\theta) = - \displaystyle{\sum ^{|V|}_{j=1}}y_{t, j} \log \hat y_{t,j})ê°€ ì£¼ë¡œ ì´ìš©ë¨

    - RNNì˜ ì¥ì : input sentenceë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìœ¼ë©°, model sizeê°€ ì»¤ì§€ì§€ ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ vanishing gradient ë¬¸ì œë¡œ long-rangeì—ì„œ ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤.

    - Gradient flow in RNN: \frac{\partial h_t}{\partial h_{t-1}} = \tanh'(W_{hh}h_{t-1}+W_{xh}x_{t})W_{hh}ë¡œ ì´ê±¸ ê³„ì† ë°˜ë³µí•˜ê²Œ ë˜ë©´ |W_{hh}| ê°€ ê³„ì† ê³±í•´ì§€ê²Œ ë˜ëŠ”ë° ì–˜ê°€ 0ì— ìˆ˜ë ´í•˜ê±°ë‚˜ ë°œì‚°í•œë‹¤ë©´ learningì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì„ ê²ƒ

    - LSTM: Long Short Term Memory

    	- cell state(c_{t-1}) : new set of hidden stateë¡œ, detouring FC layer

    	- forget gate: ì–¼ë§ˆë‚˜ ê¸°ì–µí•´ë‚¼ì§€ë¥¼ controlí•˜ëŠ” gate

    	- input gate: ë°©ê¸ˆ ì…ë ¥ëœ ë°ì´í„°ê°€ ê¸°ì–µí•  ê°€ì¹˜ê°€ ìˆëŠ”ì§€ë¥¼ control

    	- output gate: ë‹¨ê¸°ê¸°ì–µìœ¼ë¡œ ë„£ì–´ì¤˜ì•¼ í•˜ëŠ”ê²Œ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•¨

    	```undefined
    	f_t = \sigma (W_fx_t + U_f h_{t-1} + b_f)\\i_t = \sigma (W_ix_t + U_i h_{t-1} + b_i)\\o_t = \sigma (W_ox_t + U_o h_{t-1} + b_o)\\c_t = f_t \circ c_{t-1} + i_t \circ \tanh (W_cx_t + U_c h_{t-1} + b_c)\\h_t = o_t \circ \tanh (c_t)
    	```

    - GRU has fewer parameters and no additional cell state. They use a convex combination of the previous hidden state and the new one computed from the input.

    	```undefined
    	r_t = \sigma (W_rx_t + U_r h_{t-1} + b_r)\\z_t = \sigma (W_zx_t + U_z h_{t-1} + b_z)\\h_t = (1-z_t) \circ h_{t-1} + z_t \circ \tanh (W_hx_t + U_h(r_t \circ h_{t-1}) + b_h)
    	```

    - Machine Transform problemsì—ì„œ RNNì„ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì´ìœ : ë¬¸ì¥ì˜ ê¸¸ì´ì™€ ì–´ìˆœì´ ë‹¤ë¥´ê¸° ë•Œë¬¸

    - Encoder-Decoder Structure: By encoder, make an embedding containing semantics of the entire input sequence, then auto-regressive decoderì„ ë§Œë“¦

    	- <SOS>: A special token indicating the first time step

    	- At training, we use ground truth y_{t-1} as input, but at inference, we actually feed previous output

    	```python
    	from torchtext.vocab import build_vocab_from_iterator
    	
    	def tokens(data_iter):
    	    for _, text in data_iter:
    	        yield tokenizer(text)
    	
    	train_iterator = tokens(train_data)
    	encoder = build_vocab_from_iterator(train_iterator, specials=[""])
    	encoder.set_default_index(encoder[""])
    	
    	encoder(tokenizer("Hello World!"))
    	```

***

# Attention Mechanism & Transformers

    - Limitations of RNN-based models: information loss is unavoidable for long sequences

    - Attention function: Attention(Q, K,V) = value

    - Q, KëŠ” comparableí•´ì•¼ í•˜ë©°, Vì™€ attention valueëŠ” ê°™ì€ dimensionalityì—¬ì•¼ í•œë‹¤. ë§ì€ ì˜ˆì‹œì—ì„œ 4ê°œ ëª¨ë‘ê°€ ê°™ì€ dimensionì„ ê°€ì§

    - Seq2Seq modelê³¼ ë¹„êµí•˜ë©´ QëŠ” decoder hidden state, KëŠ” encoder hidden state, VëŠ” encoder hidden state

    - Dot-Product Attention

    	- encoder hidden state h_1, ...,h_T \in \reals ^h, decoder state at t is s_t \in \reals ^h

    	- Attention score e_t = \begin{bmatrix}s_t^\top h_1 &,..., & s_t^\top h_T \end{bmatrix} \in \reals^Tê°€ ë¨

    	- ì—¬ê¸°ì— softmaxë¥¼ ì ìš©í•´ ë§Œë“  attention coefficientë¥¼ encodere hidden stateì— ë„£ìœ¼ë©´ a_t = \sum_{i=1}^T [\alpha_t]_ih_i \in \reals^hì´ê³  ì´ë¥¼ decoder hidden stateì™€ concatí•´ì„œ ë¦¬í„´í•¨

    - Transformer: Attention is what you all need.

    	- Encoder

    		- Input is sequence of tokens

    		- Contextulixing the word embedding by self attention, calculate {softmax} {(\frac {Q \times K^\top}{\sqrt{d_k}})}V = Zë¥¼ multi-headì— ëŒ€í•´ í•œ í›„ concatenateí•¨

    		- Goes through an additional FC layer. FFN(x) = \max (0, xW_1+b_1)W_2+b_2ë¡œ output is still same size contextualized token embedding. ì´ì œ self-attention blockì´ stackë˜ì–´ë²„ë¦¼, ì´ ë•Œ positional encodingì„ ì¶”ê°€í•´ ì£¼ì–´ ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ íŒŒì•…ì‹œí‚´

    	- Decoder

    		- Input: given Z = \begin{Bmatrix} z_1, &..., &z_n \end{Bmatrix}, generate auto-regressively

    		- Masked Multi-head self attention: input sequence is fed into multi-head self attention layer as the encoder, and they are masked out.

    		- Encoder-decoder attention: encoder outputì„ ê°€ì§€ê³  ì˜¤ë©°, Q is the query from decoder, K, V is the key and value form encoderì„ ì´ ë•Œ maskingì€ ì—†ìŒ

    		- Feed forward layer â†’ Linear layer â†’ softmax layer

    - BERT: Bidirectional Encoder Representations from Transformersë¡œ, transformerì„ ì´ìš©í•œ ì–¸ì–´ ëª¨ë¸ì´ë‹¤. 2ê°œì˜ ë¬¸ì¥ì´ ë“¤ì–´ê°€ê³  ì–˜ë„¤ê°€ ì—°ê²°ë  ë•Œ sep í† í°, ì²« ë¬¸ì¥ì˜ ì‹œì‘ì—ëŠ” CLS í† í°ì´ ë“¤ì–´ê°

    	- example: Masked Language Modeling, Next sentence prediction
