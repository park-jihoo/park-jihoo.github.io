
# Frequent Itemset Mining & Assocation Rules

	- A general many-to-many mapping between two kinds of things
	- **Support** : Number of baskes containing all items in $I$
	- **Confidence**: $\frac {\textbf{support}(I\cup j)}{\textbf{support}(I)}$
	- **Interest** : difference between its confidence and the fraction of baskets that contain $j$

		$$
		\textbf{Interest}(I \rightarrow j) = \textbf{conf}(I \rightarrow j) - \Pr[j]
		$$

	- Goal : Find all association rulses with support$\geq s$ and confidence $\geq c$
	- Mining Association Rules
		1. Find all frequent itemset I
		2. Rule generation
			1. For every subset A of I, generate a reule A â†’ I \ A
			2. Output the rules above the confidence threshold
	- Compacting the output
		- Maximal frequent itemsets
		- Closed itemsets

	## Finding Frequent Itemsets

		- Computation model
			- The true cost of mining disk-resident data is usually the number of **disk I/O**
			- Association-rule algorithms read the data in passes - all baskets read in turn
			- We measure the cost by the **number of passses** an nalgorithm makes over the data
		- Main memory bottleneck: # of different things we can count is limited by main memory
		- Finding Frequent pairs: pairs are common
			1. Naive Algorithm: generate $\frac {n(n-1)}{2}$ pairs
			2. Count all pairs using matrix(4 bytes per pair)
			3. Keep a table of triples with count(only pairs with count > 0, 12 bytes per pair)

		### A-Priori Algorithm


			> ğŸ’¡ Key Idea : **monotonicity**

			1. Read baskets and count in main meory the occurences of each individual item
			2. Read baskets again and count in main memory <u>only</u> those pairs where both elements are frequent

			```mermaid
			graph LR
				subgraph Pass1
					A(item counts)
				end
				subgraph pass2
					B(Frequent items)
					C(Counts of pairs of frequent items)	
				end
			A ---> B
			```

			- In frequent triples and more, for each k we construct to set of k tuples.
				- Candidate k tuples: those that might be frequent sets based on information from the pass for k -1
				- The set of truly frequent k tuples

		### PCY Algorithm


			> ğŸ’¡ Use idle memory to reduce memory required in pass 2 of A-priori!

			- Pass 1 of PCY: In addition to item counts, maintain a hash table with as many buckets as fit in memory

			```text
			FOR (each basket):
				FOR (each item in the basket):
					add 1 to item's count
				FOR (each pair of items):
					hash the pair to a bucket
					add 1 to the count for that bucket
			```

			- Pass 2: Only count pairs that hash to frequent buckets
			- Between passes - replace the buckets by bit0vector, then 4 byte integers will be replaced to bits and it requires 1/32 of memory
		- Multistage Algorithm: limit the number of candidates to be counted
			- After pass 1 of PCY, rehash only those pairs that qualify for pass 2 of PCY
			- On middle pass, fewer pass contribute to buckets, which leads to fewer positives
		- Multihash: Use Several independent hash tables on the first pass
			- Risk: Halving the number of buckets doubles the average count

		### Frequent itemsets in â‰¤2 passes

			- Random sampling: take a random sample of the market baskets and run a-priori or one of its improvements in main memory
			- SON: Repeatedlyl read small subsets of the baskets into main memory and run an in-memory algorithm to find all frequent itemsets

# Finding Similar Items: LSH


	> ğŸ’¡ Given q, find data points x_j that are within distance threshold d(q, x_j) â‰¤ s in O(1)

	- Distance Measures: Jaccard distance/similarity

		$$
		sim(C_1, C_2) = |C_1 \cap C_2| / |C_1 \cup C_2| \\ d(C_1, C_2) = 1 - sim(C_1, C_2)
		$$

	- 3 Essential steps for similar docs
		1. Shingling: Convert Document to sets
		2. Min-hashing: Convert Large sets to short signatures, while preserving similarity
		3. LSH: Focus on pairs of signatures likely to be from simlilar documents

	## Shingling

		- A k-shingle for a document is a sequence of k tokens that appears in the doc
		- Document $D_1$ is a set of its k-shingles $C_1 = S(D_1)$ and each document is a 0/1 vector in the space of k-shingles
		- Documents that have lots of shingles in common have similar text, even if the text appears in different order

	## MinHashing

		- Encode sets using 0/1 vectors
		- Interpret set intersection as bitwise AND, and set union as bitwise OR
		- Rows are elements(shingles), columns are sets(documents)
		- Hashing columns: find a hash function such that if similarity is high then with high probability same hash function value, and low then high probability different hash function value
		- Min-hashing: define a hash function $h_\pi(C) = \min_\pi\pi(C)$

		$$
		\Pr[h_\pi(C_1) = h_\pi(C_2)] = sim(C_1, C_2)
		$$

		- Similarity of 2 signatures is the fraction of the hash functions in which they agree
		- $sig(C)[i] = \min (\pi_i(c))$ is compressed bit vector

	## Locality Sensitive Hashing


		> ğŸ’¡ Goal: Find documents with Jaccard similarity at least s

		- Ensure that similar columns are likely to hash to the same bucket with high probability
		- Overview of LSH
			1. Divide matrix M into b bands of r rows
			2. **For each band**, hash its protion of each column to a hash table with k buckets(make k as large as possible)
			3. Candidate column pairs are those that hash to the same bucket for â‰¥1 band
			4. Tune b and r to catch most similar pairs, but few non-similar pairs
		- (For simplicity) We assume that same bucket implies identical in that band
		- Tradeoff: The number of b and number of rows r balances false positive / false negative
		- Assume column C1 and C2 have similarity t
			- Prob that all rows in band equal $t^r$
			- Prob that some row in band unequal $1 - t^r$
			- Prob that no band is identical $(1-t^r)^b$
			- Prob that at least 1 band identical $1-(1-t^r)^b$
	- Generalizing Min-hash
	- Choosing s-curve by r and b
	- Probability that c1 and c2 are candidate pair is $1 - (1-s^r)^b$

	![Untitled.png](https://s3.us-west-2.amazonaws.com/secure.notion-static.com/39e0b223-1835-42aa-b402-55c900b7fe90/Untitled.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45EIPT3X45%2F20231007%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20231007T164653Z&X-Amz-Expires=3600&X-Amz-Signature=91cacfc760fb3eec0c857f21cbf0ae5b6a5171e6eb9af045e6c5806b3ea55415&X-Amz-SignedHeaders=host&x-id=GetObject)

	- bê°€ ê³ ì •ë˜ì–´ ìˆì„ ë•Œ, rì´ ì»¤ì§ˆìˆ˜ë¡ false negative ì¦ê°€, false positive ê°ì†Œ
	- rì´ ê³ ì •ë˜ì–´ ìˆì„ ë–„, bê°€ ì»¤ì§ˆìˆ˜ë¡ false positive ì¦ê°€, false negative ê°ì†Œ

	## LS Families

		- A family H of hash function is said to be $(d_1, d_2, p_1, p_2)$-sensitive if for any x and y in s
			- if $d(x, y) \leq d_1$ then probability over all $h \in H$ that h(x) = h(y) is at least $p_1$
			- if $d(x, y) \geq d_2$ then probability over all $h \in H$ that h(x) = h(y) is at most $p_2$

		> With LS Families we can to LSH

	- Amplifying Hash functions: AND and OR
		- AND construction like **rows in a band**
		- OR construction like **many bands**
		- í•©ì¹˜ë©´ ê²°êµ­ $1 - (1-s^r)^b$ê°€ ë‚˜ì™€ì„œ LSH

# Link Analysis, PageRank

	- How to organize Web?
		- Human created web directories
		- Web search
	- 2 challenges of web search
		1. Who to **trust**?
		2. What is the **best** answer to query?
	- Link Analysis algorithmsâ€™ idea: Links as votes
	- _Random surfer_ model: Start at random page and follow random outlinks repeatedly, from hatever page you are at
	- importance = the principal eigenvector of the transition matrix of the Web
	- rank for page

	$$
	r_j = \sum_{i \rightarrow j} \frac {r_i}{d_i}
	$$

	- Stochastic adjacency matrix M: if iâ†’j then $M_{ji} = \frac{1}{d_i}$ else 0
	- Rank vector r: vector with an entry per page
	- The flow equations can be written as

		$$
		r = M \cdot r
		$$

	- We can solve this by power iteration!
	- Imagine a random walk surfer: at time t+1, the surfer follows an out-link from i uniformly at random and ends up some page j linked from i
	- If itâ€™s stationary distribution, $p(t+1) = M \cdot p(t) = p(t)$
	- In undirected graph, $r_i = d_i/2m$
	- Two problems: Dead end, spider traps â†’ teleport!
	- Random teleport, then equation changes like this

		$$
		r_j = \sum_{i \rightarrow j} \frac {r_i}{d_i} + (1-\beta)\frac{1}{N}
		$$

	- Google Matrix A

		$$
		A = \beta M + (1-\beta) \left[ \frac{1}{N} \right]_{N\times N}
		$$

	- Sparse Matrix Encoding: encode sparse matrix using only nonzero entries â†’ $2|r|+|M|$
	- Block-based Update Algorithm â†’ break r_new into k blocks that fit in memory â†’ $k(|M|+|r|)+|r|$
	- Block-stripe Update Algorithm â†’ break M into stripes! Each stripe only contain destinatioin nodes in corresponding block of r new â†’ $|M| (1+\epsilon) + (k+1)|r|$

	### Topic-specific Pagerank

		- Instead of generic popularity, we can measure popularity withun a topic.
		- Teleport can go to topic speciific set of relevant pages
			- Bias the random walk(topic specific set of relevant pages)
		- Which topic ranking to use? User context
		- Graph proximity: ì¼ë°˜ì ì¸ ê·¸ë˜í”„ ìƒì—ì„œì˜ ìœ„ì¹˜
		- Simrank: Random walks from a fixed node on k-partite graph, then teleport set S = {u}

	### TrustRank: combating web spam

		- First Spammer: Term spam, and google decided pagerank as a tool to measure the importance of Web pages
		- Spam Farming: Spam farms were developed to concentrate pageRank on a single page, and there is a link structures that boost pagerank of particular page

		$$
		y = \frac {x}{1-\beta^2}+c \frac{M}{N}, c = \frac{\beta}{1+\beta}
		$$

		- Trustrank: topic-specific pagerank with teleport set ot **trusted pages**
			- Sample a set of seed pages from the web
			- Use a threshold value and mark all pages below the trust threshold as spam
			- Trust propagation: for each $q \in o_p$, p confers to the trust to q is  $\frac{\beta t_p}{|o_p|}$. This trust is additive
		- Spam Mass = $\frac {r_p - r_p ^+}{r_p}$, $r_p$ is pagerank of p, and $r_p^+$is pagerank of p with teleport into trusted pages only

	### HITS

		- Quality as an expert(hub), quality as a content(authority)
		- Authorities are pages containing useful informations
		- Hubs are pages that link to authorities

		```python
		a[0], h[0] = 1/sqrt(N), 1/sqrt(N)
		while converge:
			a[t+1] = sum(h[t])
			h[t+1] = sum(a[t])
			normalize a, h
		```

		- Write vector a and h as authority and hubs and adjacency matrix as $A_{ij}=1$ if iâ†’j, 0 otherwise then

		$$
		a = (A^TA)a \\ h = (AA^T)h
		$$


# Community Detection in Graphs


	> ğŸ’¡ PageRank based Algorithm for finding dense clusters

	- Seed Nodes: Compute approximate Personalized Pagerank(PPR) around node s
	- Algorithm outline
		- Pick a seed node s of interest
		- Run PPR with teleport set {s}
		- Sort the nodes by the decreasing PPR score
		- Sweep overr the nodes and find **good clusters**
	- Cut

	$$
	cut(A) = \sum_{i \in A, j \notin A} w_{ij}
	$$

	- Graph Partitioning Criteria: conductance

	$$
	\phi (A) = \frac {|\{(i, j) \in E ; i \in A, j\notin A \}|}{\min(vol(A), 2m-vol(A))}
	$$

	- Sweep: sort nodes by decreasing ppr score and get local minima of conductance
	- Computing PPR

		$$
		r^{(t+1)} = \beta M \cdot r^{(t)} + (1 - \beta) a
		$$

	- Approximate pagerank: lazy random walk, stays in 1/2 probability and walks to random neighbor at other half
	- An $\epsilon$-approximate Pagerank vector for $pr_\alpha(s)$ is a pagerank vector $pr_\alpha(s-r)$ where the vector r is nonnegative and satisfies $r(v) \leq \epsilon d(v)$ for every vertex v in the graph
	- Push algorithm

		```python
		r', q' = r, q
		r'[u] = r[u] + (1-beta)q[u]
		q'[u] = beta * q[u] / 2
		for v in u->v:
			q'[v] = q[v] + beta * q[u] / (2 * d[u])
		return r', q'
		```

	- Why standard cluster methods do not work?
		1. high dimensional data
		2. noisy and sparse data
		3. number of cluster is unknown
		4. cell types are hierarchically organized
	- Knn graph: directed graph with vertex set V and an edge from v to its k most similar objects in V
	- NN-descent: if K is large enough then even if we start from a arandom knn approximation we are likely to find for each object K times with radius of
	- NN-descent: if K is large enough then even if we start from a arandom knn approximation we are likely to find for each object K times with radius of âˆ†/2 by exploring its neighborsâ€™ neighbor
		1. Start with a random knn graph
		2. iteratively refine the list of nearest neighbors of each code
		3. keep doing until convergence
	- Modularaity Q

		$$
		Q(G, S) = \frac 1 {2m} \sum_{s \in S} \sum_{i\in S} \sum_{j\in S} \left( A_{ij} - \frac {k_ik_j}{2m} \right) \\Q = \frac 1{2m} \sum_{ij} \left[ A_{ij} - \frac {k_ik_j}{2m}  \right] \delta(c_i, c_j)
		$$

		- We can identify communities by maximizing modularity
	- Louvain algorithm: greedy algorithm for community detection
		1. Modularity is optimized by allowing only local changes to node-communities memberships
		2. The identified communities are aggregated into super-nodes to build a new network
		3. Repeat until no increase of modularity is possible

		$$
		\Delta Q(i \rightarrow C) = \left[ \frac {\sum_{in} + k_{i, in}}{2m} - \left(\frac{\sum_{tot}+k_i}{2m} \right)^2\right] - \left[ \frac {\sum_{in} }{2m} - \left(\frac{\sum_{tot}}{2m} \right)^2 - \left(\frac{k_i}{2m} \right)^2\right]
		$$


# Recommender System

	1. Gathering known ratings for matrix
	2. Extrapolating unknown ratings from the known ones
	3. Evaluating extrapolation methods

	## Content-based


		> ğŸ’¡ Recommend items to customer x similar to previous items rated highly by x

		- How to pick important features? TF-IDF(Term frequency * inverse doc frequency)
		- User profile possibilities: $u(x, i) = \cos (x, i) = \frac {x\cdot i}{||x||\cdot ||i||}$
- Pros
	- No need for data on other users
	- Able to recommend to users with unique tastes
	- Able to recommend new items
	- Able to provide explanations
- Cons
	- Finding the appropriate features is hard
	- Recommendations for new users
	- Overspecialization

	## Collaborative filtering

		- User-user collaborative filtering, Item-item collaborative filtering
		- Find similiar users by pearson correlation coefficient

			$$
			sim(x, y) = \frac {\sum_{s\in S_{xy}} (r_{xs}-\bar{r_x})(r_{ys}-\bar{r_y})}{\sqrt{\sum_{s\in S{xy}}(r_{xs}-\bar{r_x})^2}\sqrt{\sum_{s\in S{xy}}(r_{ys}-\bar{r_y})^2}}
			$$

		- Rating prediction $r_{xi} = \frac 1 k \sum_{y\in N} r_{yi}$
		- Item-item collaborative filtering

			$$
			r_{xi} = b_{xi}+\frac {\sum_{j \in N(i;x)} s_{ij} \cdot (r_{xj} - b_{xj})}{\sum_{j \in N(i;x)}s_{ij}}
			$$

- Pros of collaborative filtering
	- Works for any kind of item
- Cons for collaborative filtering
	- Cold start
	- Sparsity
	- First rater
	- Popularity Bias
	- BellKor Recommender System
		- Multi-scale modeling of the data
			1. Global: overall deviations of users/movies
			2. Regional: Factorization â†’ Addressing regional effects
			3. Local: Collaborative Filtering
	- Interpolation weights: use a **weighted sum** rather than weighted avg:

		$$
		\hat {r_{xi}} = b_{xi} + \sum_{j \in N(i;x)} w_{ij}(r_{xj}-b_{xj})
		$$

		- ì—¬ê¸°ì—ì„œ SSEë¥¼ minimizeí•˜ëŠ”ë°, ì´ëŠ” gradient descentë¥¼ ì´ìš©í•´ì„œ ì¦ëª… ê°€ëŠ¥í•˜ë‹¤
	- Latent Factor Models: $R = Q \cdot P^T$, and sum of squared error is $\min_{U,V,\Sigma} \sum_{ij \in A} \left( A_{ij} - [U\Sigma V^T]_{ij} \right) ^2 = \min_{P,Q} \sum_{(i, x) \in R} (r_{xi} - q_i \cdot p_x)^2$
	- ì—¬ê¸°ì— regularization termì„ ì¶”ê°€í•˜ì—¬ gradient descentë¥¼ ì ìš©í•œ ë‹¤ìŒ optimal valueë¥¼ ì°¾ëŠ”ë‹¤.

# Product Quantization

	- Quantization: the process of constraining an input from a continuous or otherwise large set of values to a discrete set
		- Efficient Memory usage
		- Lower power
		- Saving RAM memory bandwidth
		- Improvement of inference speed
		- Reduce accuracy
	- Searching with Quantization
		- Symmetric distance computation: distance between query centerioids
		- Asymmetric distance computation: distance between x and queried y
			1. Train k centroids for each sub vector spaces with k means algorithm
			2. Substitute with the id of the nearest centroid from the respective sub-vector spaces
			3. calculate distence between query sub vector and every centroid
			4. For each row of PQ code, look up the partial distances and sum them
	- Inverted File System with asymetric distance
		- Inverted index: database index storing a mapping from content to its locations
		- Partitioning with K-mean clustering: the search can only be performed on vectors belonging to specific partitions
		- Why using residual here? Residual is more compact distribution and uses lower variance
